# Proximal Policy Optimization
This repo combined PPO1 (use KL-divergence to control ratio) and PPO2 (only use clipped surrogate lose to control). Refer to *agent.py* to see the details and it can be changed to PPO1 or PPO2 with a little motificaitons.
